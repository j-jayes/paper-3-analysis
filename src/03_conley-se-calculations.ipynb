{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "root = pathlib.Path.cwd().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 20.5 GiB for an array with shape (1000, 2753469) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\User\\Documents\\Recon\\paper-3-analysis\\src\\03_conley-se-calculations.ipynb Cell 2\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Recon/paper-3-analysis/src/03_conley-se-calculations.ipynb#W0sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mConley SE:\u001b[39m\u001b[39m\"\u001b[39m, conley_se)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Recon/paper-3-analysis/src/03_conley-se-calculations.ipynb#W0sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Recon/paper-3-analysis/src/03_conley-se-calculations.ipynb#W0sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m     main()\n",
      "\u001b[1;32mc:\\Users\\User\\Documents\\Recon\\paper-3-analysis\\src\\03_conley-se-calculations.ipynb Cell 2\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Recon/paper-3-analysis/src/03_conley-se-calculations.ipynb#W0sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m merged_data \u001b[39m=\u001b[39m compute_centroids(merged_data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Recon/paper-3-analysis/src/03_conley-se-calculations.ipynb#W0sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m distance_threshold \u001b[39m=\u001b[39m \u001b[39m10000\u001b[39m  \u001b[39m# 10 kilometers\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Recon/paper-3-analysis/src/03_conley-se-calculations.ipynb#W0sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m weights \u001b[39m=\u001b[39m construct_spatial_weights_chunked(merged_data, bandwidth_multiplier\u001b[39m=\u001b[39;49m\u001b[39m1.0\u001b[39;49m, distance_threshold\u001b[39m=\u001b[39;49mdistance_threshold, chunk_size\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Recon/paper-3-analysis/src/03_conley-se-calculations.ipynb#W0sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m independent_vars \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mbirth_parish_treated\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mage\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mage_2\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfemale\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Recon/paper-3-analysis/src/03_conley-se-calculations.ipynb#W0sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m coefficients, conley_se \u001b[39m=\u001b[39m conley_se_regression(merged_data, weights, independent_vars)\n",
      "\u001b[1;32mc:\\Users\\User\\Documents\\Recon\\paper-3-analysis\\src\\03_conley-se-calculations.ipynb Cell 2\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Recon/paper-3-analysis/src/03_conley-se-calculations.ipynb#W0sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, num_coords, chunk_size):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Recon/paper-3-analysis/src/03_conley-se-calculations.ipynb#W0sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     end \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(i \u001b[39m+\u001b[39m chunk_size, num_coords)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Recon/paper-3-analysis/src/03_conley-se-calculations.ipynb#W0sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     chunk_distances \u001b[39m=\u001b[39m pairwise_distances(coords[i:end], coords)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Recon/paper-3-analysis/src/03_conley-se-calculations.ipynb#W0sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39mif\u001b[39;00m distance_threshold:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Recon/paper-3-analysis/src/03_conley-se-calculations.ipynb#W0sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m         chunk_distances[chunk_distances \u001b[39m>\u001b[39m distance_threshold] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Recon\\paper-3-analysis\\.venv\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:2195\u001b[0m, in \u001b[0;36mpairwise_distances\u001b[1;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001b[0m\n\u001b[0;32m   2192\u001b[0m         \u001b[39mreturn\u001b[39;00m distance\u001b[39m.\u001b[39msquareform(distance\u001b[39m.\u001b[39mpdist(X, metric\u001b[39m=\u001b[39mmetric, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds))\n\u001b[0;32m   2193\u001b[0m     func \u001b[39m=\u001b[39m partial(distance\u001b[39m.\u001b[39mcdist, metric\u001b[39m=\u001b[39mmetric, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m-> 2195\u001b[0m \u001b[39mreturn\u001b[39;00m _parallel_pairwise(X, Y, func, n_jobs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Recon\\paper-3-analysis\\.venv\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1765\u001b[0m, in \u001b[0;36m_parallel_pairwise\u001b[1;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[0;32m   1762\u001b[0m X, Y, dtype \u001b[39m=\u001b[39m _return_float_dtype(X, Y)\n\u001b[0;32m   1764\u001b[0m \u001b[39mif\u001b[39;00m effective_n_jobs(n_jobs) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m-> 1765\u001b[0m     \u001b[39mreturn\u001b[39;00m func(X, Y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m   1767\u001b[0m \u001b[39m# enforce a threading backend to prevent data communication overhead\u001b[39;00m\n\u001b[0;32m   1768\u001b[0m fd \u001b[39m=\u001b[39m delayed(_dist_wrapper)\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Recon\\paper-3-analysis\\.venv\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:338\u001b[0m, in \u001b[0;36meuclidean_distances\u001b[1;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[0;32m    332\u001b[0m     \u001b[39mif\u001b[39;00m Y_norm_squared\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m (\u001b[39m1\u001b[39m, Y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[0;32m    333\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    334\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIncompatible dimensions for Y of shape \u001b[39m\u001b[39m{\u001b[39;00mY\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    335\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mY_norm_squared of shape \u001b[39m\u001b[39m{\u001b[39;00moriginal_shape\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    336\u001b[0m         )\n\u001b[1;32m--> 338\u001b[0m \u001b[39mreturn\u001b[39;00m _euclidean_distances(X, Y, X_norm_squared, Y_norm_squared, squared)\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Recon\\paper-3-analysis\\.venv\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:379\u001b[0m, in \u001b[0;36m_euclidean_distances\u001b[1;34m(X, Y, X_norm_squared, Y_norm_squared, squared)\u001b[0m\n\u001b[0;32m    376\u001b[0m     distances \u001b[39m=\u001b[39m _euclidean_distances_upcast(X, XX, Y, YY)\n\u001b[0;32m    377\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    378\u001b[0m     \u001b[39m# if dtype is already float64, no need to chunk and upcast\u001b[39;00m\n\u001b[1;32m--> 379\u001b[0m     distances \u001b[39m=\u001b[39m \u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m \u001b[39m*\u001b[39;49m safe_sparse_dot(X, Y\u001b[39m.\u001b[39;49mT, dense_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    380\u001b[0m     distances \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m XX\n\u001b[0;32m    381\u001b[0m     distances \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m YY\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 20.5 GiB for an array with shape (1000, 2753469) and data type float64"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "def load_data(census_filepath, geojson_filepath):\n",
    "    # Load the census data\n",
    "    census_data = pd.read_parquet(census_filepath) \n",
    "    # Load the GeoJSON data into a GeoDataFrame\n",
    "    parishes_gdf = gpd.read_file(geojson_filepath)\n",
    "    \n",
    "    return census_data, parishes_gdf\n",
    "\n",
    "def transform_crs(parishes_gdf):\n",
    "    return parishes_gdf.to_crs(epsg=3006)\n",
    "\n",
    "def filter_parishes(census_data, parishes_gdf):\n",
    "    relevant_parishes = census_data['current_parish_ref_code'].unique()\n",
    "    return parishes_gdf[parishes_gdf['ref_code'].isin(relevant_parishes)]\n",
    "\n",
    "def merge_data(census_data, parishes_gdf):\n",
    "    merged_data = census_data.merge(parishes_gdf, left_on=\"current_parish_ref_code\", right_on=\"ref_code\", how=\"left\")\n",
    "    merged_data = gpd.GeoDataFrame(merged_data, geometry='geometry')\n",
    "    return merged_data\n",
    "\n",
    "def compute_centroids(merged_data):\n",
    "    merged_data['centroid'] = merged_data.geometry.centroid\n",
    "    return merged_data\n",
    "\n",
    "def construct_spatial_weights_chunked(merged_data, bandwidth_multiplier=1.0, distance_threshold=None, chunk_size=1000):\n",
    "    coords = merged_data['centroid'].apply(lambda x: (x.x, x.y)).tolist()\n",
    "    num_coords = len(coords)\n",
    "    weights_sparse = csr_matrix((num_coords, num_coords), dtype=np.float64)\n",
    "    for i in range(0, num_coords, chunk_size):\n",
    "        end = min(i + chunk_size, num_coords)\n",
    "        chunk_distances = pairwise_distances(coords[i:end], coords)\n",
    "        if distance_threshold:\n",
    "            chunk_distances[chunk_distances > distance_threshold] = 0\n",
    "        bandwidth = chunk_distances[chunk_distances != 0].mean() * bandwidth_multiplier\n",
    "        chunk_weights = np.where(chunk_distances != 0, 1 / (chunk_distances / bandwidth), 0)\n",
    "        weights_sparse[i:end] = csr_matrix(chunk_weights)\n",
    "    return weights_sparse\n",
    "\n",
    "def conley_se_regression(merged_data, weights, independent_vars):\n",
    "    X = merged_data[independent_vars]\n",
    "    X = sm.add_constant(X)\n",
    "    y = merged_data['employed']\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    residuals = model.resid\n",
    "    spatial_residuals = weights @ residuals\n",
    "    spatial_residuals = np.array(spatial_residuals).reshape(-1)\n",
    "    X_transpose_W = X.T * spatial_residuals\n",
    "    vcov = X_transpose_W @ X / len(y)\n",
    "    conley_se = np.sqrt(np.diag(vcov))\n",
    "    return model.params, conley_se\n",
    "\n",
    "def main():\n",
    "    census_filepath = root / \"data/census/census_1930.parquet\"\n",
    "    geojson_filepath = root/ \"data/maps/Swedish_parishes_1926.geojson\"\n",
    "\n",
    "    census_data, parishes_gdf = load_data(census_filepath, geojson_filepath)\n",
    "    parishes_gdf = transform_crs(parishes_gdf)\n",
    "    parishes_gdf = filter_parishes(census_data, parishes_gdf)\n",
    "    merged_data = merge_data(census_data, parishes_gdf)\n",
    "    merged_data = compute_centroids(merged_data)\n",
    "    distance_threshold = 10000  # 10 kilometers\n",
    "    weights = construct_spatial_weights_chunked(merged_data, bandwidth_multiplier=1.0, distance_threshold=distance_threshold, chunk_size=1000)\n",
    "    independent_vars = ['birth_parish_treated', 'age', 'age_2', 'female']\n",
    "    coefficients, conley_se = conley_se_regression(merged_data, weights, independent_vars)\n",
    "    print(\"Coefficients:\", coefficients)\n",
    "    print(\"Conley SE:\", conley_se)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Results: Ordinary least squares\n",
      "====================================================================\n",
      "Model:                OLS              Adj. R-squared:     0.001    \n",
      "Dependent Variable:   employed         AIC:                -321.2452\n",
      "Date:                 2023-10-16 13:50 BIC:                -311.1312\n",
      "No. Observations:     1161             Log-Likelihood:     162.62   \n",
      "Df Model:             1                F-statistic:        2.713    \n",
      "Df Residuals:         1159             Prob (F-statistic): 0.0998   \n",
      "R-squared:            0.002            Scale:              0.044321 \n",
      "--------------------------------------------------------------------\n",
      "                      Coef.  Std.Err.    t     P>|t|   [0.025 0.975]\n",
      "--------------------------------------------------------------------\n",
      "birth_parish_treated -0.0483   0.0293  -1.6472 0.0998 -0.1059 0.0092\n",
      "const                 0.9557   0.0063 151.0451 0.0000  0.9433 0.9682\n",
      "--------------------------------------------------------------------\n",
      "Omnibus:              1012.810      Durbin-Watson:         1.990    \n",
      "Prob(Omnibus):        0.000         Jarque-Bera (JB):      16709.059\n",
      "Skew:                 -4.293        Prob(JB):              0.000    \n",
      "Kurtosis:             19.483        Condition No.:         5        \n",
      "====================================================================\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors\n",
      "is correctly specified.\n",
      "Conley Standard Errors: [0.         0.03823778]\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from shapely.geometry import box\n",
    "\n",
    "def load_and_filter_data(geojson_path, parquet_path):\n",
    "    \"\"\"\n",
    "    Load the GeoJSON and Excel data, merge them based on the specified columns, \n",
    "    and filter the parishes based on the sample.\n",
    "    \n",
    "    Args:\n",
    "    - geojson_path (str): Path to the GeoJSON file.\n",
    "    - excel_path (str): Path to the Excel file with census data.\n",
    "    \n",
    "    Returns:\n",
    "    - GeoDataFrame: Merged and filtered GeoDataFrame.\n",
    "    \"\"\"\n",
    "    # Load GeoJSON data\n",
    "    gdf = gpd.read_file(geojson_path)\n",
    "    \n",
    "    # Load Excel data\n",
    "    census_data = pd.read_parquet(parquet_path)\n",
    "    \n",
    "    # Join the GeoJSON data with the census data\n",
    "    merged_gdf = gdf.merge(census_data, left_on='ref_code', right_on='current_parish_ref_code', how='inner')\n",
    "    \n",
    "    # Add a constant column for regression\n",
    "    merged_gdf[\"const\"] = 1\n",
    "    \n",
    "    # Filter the GeoDataFrame to only include parishes that appear in the sample\n",
    "    filtered_gdf = merged_gdf.drop_duplicates(subset=['ref_code'])\n",
    "    \n",
    "    return filtered_gdf\n",
    "\n",
    "def compute_spatial_matrix(filtered_gdf):\n",
    "    \"\"\"\n",
    "    Compute the spatial weighting matrix using queen contiguity.\n",
    "    \n",
    "    Args:\n",
    "    - filtered_gdf (GeoDataFrame): The filtered GeoDataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    - ndarray: Spatial weighting matrix.\n",
    "    \"\"\"\n",
    "    # Compute bounding boxes for each parish\n",
    "    filtered_gdf['bbox'] = filtered_gdf.geometry.apply(lambda x: box(*x.bounds))\n",
    "    \n",
    "    n = len(filtered_gdf)\n",
    "    W = np.zeros((n, n))\n",
    "    index_mapping = {idx: i for i, idx in enumerate(filtered_gdf.index)}\n",
    "    \n",
    "    # Identify potential neighbors based on bounding box intersections and refine the list\n",
    "    for idx, row in filtered_gdf.iterrows():\n",
    "        i = index_mapping[idx]\n",
    "        potential_neighbors = [index_mapping[potential_idx] for potential_idx in filtered_gdf[filtered_gdf.bbox.intersects(row.bbox)].index]\n",
    "        for j in potential_neighbors:\n",
    "            if i != j and (row.geometry.touches(filtered_gdf.loc[filtered_gdf.index[j], 'geometry']) or\n",
    "                           row.geometry.intersects(filtered_gdf.loc[filtered_gdf.index[j], 'geometry'])):\n",
    "                W[i, j] = 1\n",
    "    \n",
    "    return W\n",
    "\n",
    "def run_regression_with_conley_se(filtered_gdf, W, outcome_var, independent_var):\n",
    "    \"\"\"\n",
    "    Run a regression and compute Conley standard errors using the spatial weighting matrix.\n",
    "    \n",
    "    Args:\n",
    "    - filtered_gdf (GeoDataFrame): The filtered GeoDataFrame.\n",
    "    - W (ndarray): Spatial weighting matrix.\n",
    "    - outcome_var (str): Name of the outcome variable.\n",
    "    - independent_var (str): Name of the independent variable.\n",
    "    \n",
    "    Returns:\n",
    "    - OLSResults: Regression results.\n",
    "    - ndarray: Conley standard errors.\n",
    "    \"\"\"\n",
    "    regression_data = filtered_gdf[[outcome_var, independent_var, \"const\"]]\n",
    "    \n",
    "    # Specify the regression model\n",
    "    model = sm.OLS(regression_data[outcome_var], regression_data[[independent_var, \"const\"]])\n",
    "    \n",
    "    # Fit the model\n",
    "    results = model.fit()\n",
    "    \n",
    "    # Aggregate residuals by parish\n",
    "    grouped_residuals = filtered_gdf.groupby(\"ref_code\").apply(lambda x: (x[outcome_var] - results.predict(x[[independent_var, \"const\"]])).sum())\n",
    "    \n",
    "    # Calculate the matrix product of aggregated residuals and regression data\n",
    "    R_aggregated = np.outer(grouped_residuals, regression_data[[independent_var, \"const\"]].values)\n",
    "    \n",
    "    # Compute spatially weighted covariance\n",
    "    spatial_cov_aggregated = R_aggregated.T @ (W @ R_aggregated)\n",
    "    \n",
    "    # Extract the diagonal elements to get the variance for each coefficient\n",
    "    conley_se_aggregated = np.sqrt(np.diag(spatial_cov_aggregated)[:2] / len(regression_data))\n",
    "    \n",
    "    return results, conley_se_aggregated\n",
    "\n",
    "# Paths to the input files (adjust as needed)\n",
    "parquet_path = root / \"data/census/census_1930.parquet\"\n",
    "geojson_path = root/ \"data/maps/Swedish_parishes_1926.geojson\"\n",
    "\n",
    "# Load and filter data\n",
    "filtered_gdf = load_and_filter_data(geojson_path, parquet_path)\n",
    "\n",
    "# Compute spatial matrix\n",
    "W = compute_spatial_matrix(filtered_gdf)\n",
    "\n",
    "# Run regression and compute Conley SE\n",
    "outcome_variable = \"employed\"\n",
    "independent_variable = \"birth_parish_treated\"\n",
    "results, conley_se = run_regression_with_conley_se(filtered_gdf, W, outcome_variable, independent_variable)\n",
    "\n",
    "print(results.summary2())\n",
    "print(\"Conley Standard Errors:\", conley_se)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
